{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ypt7ymI8X-rM"
   },
   "source": [
    "<h1><center>Векторные представления слов</center></h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z3sSyfHaX-rN"
   },
   "source": [
    "![w2v](https://cdn-images-1.medium.com/max/2600/1*sXNXYfAqfLUeiDXPCo130w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0oH_UDyFX-rO"
   },
   "source": [
    "Есть три сценария работы с векторными представлениями:\n",
    "- Взять предобученную модель\n",
    "- Обучить свою модель \n",
    "- Взять предобученную модель и дообучить ее\n",
    "\n",
    "\n",
    "## Предобученные модели: RusVectōrēs\n",
    "\n",
    "\n",
    "На сайте [RusVectōrēs](https://rusvectores.org/ru/) собраны предобученные на различных данных модели для русского языка, а также можно поискать наиболее близкие слова к заданному, посчитать семантическую близость нескольких слов и порешать примеры с помощью «калькулятора семантической близости».\n",
    "\n",
    "\n",
    "Для других языков также можно найти предобученные модели — например, модели [fastText](https://fasttext.cc/docs/en/english-vectors.html) и [GloVe](https://nlp.stanford.edu/projects/glove/) (о них чуть дальше)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yXNm8flVX-rP"
   },
   "source": [
    "## Gensim\n",
    "\n",
    "Использовать предобученную модель или обучить свою можно с помощью библиотеки `gensim`. Вот [ее документация](https://radimrehurek.com/gensim/models/word2vec.html).\n",
    "\n",
    "### Как использовать готовую модель\n",
    "\n",
    "Модели word2vec бывают разных форматов:\n",
    "\n",
    "* .vec.gz — обычный текстовый файл \n",
    "* .bin.gz — бинарный файл\n",
    "\n",
    "Загружаются они с помощью одного и того же класса `KeyedVectors`, меняется только параметр `binary` у функции `load_word2vec_format`. \n",
    "\n",
    "Если же векторы обучены **не** с помощью word2vec, то для загрузки нужно использовать функцию `load`. Т.е. для загрузки предобученных моделей *glove, fasttext, bpe* и любых других нужна именно она.\n",
    "\n",
    "Скачаем с RusVectōrēs модель для русского языка, обученную на НКРЯ : https://rusvectores.org/ru/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "sTLnNKaNX-rP",
    "outputId": "73be423c-b644-4fb2-860c-337c966cf788"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/veronika/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import gensim\n",
    "import logging\n",
    "import nltk.data \n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "zm9z6SN3X-rS",
    "outputId": "3db5572e-b9f8-4df4-a0eb-80aa7b822692"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_embeddings-Copy1.ipynb\r\n",
      "2_embeddings_no_output.ipynb\r\n",
      "2_embeddings_with_output.ipynb\r\n",
      "IMDB Dataset.csv\r\n",
      "alice.txt\r\n",
      "cc.en.300.bin\r\n",
      "cc.en.300.bin.gz\r\n",
      "cc.ru.300.bin\r\n",
      "cc.ru.300.bin.gz\r\n",
      "clean_text.txt\r\n",
      "\u001b[34mglove.6B\u001b[m\u001b[m\r\n",
      "glove.6B.zip\r\n",
      "imdb-dataset-of-50k-movie-reviews.zip\r\n",
      "movie_reviews.model\r\n",
      "movies_alice.bin\r\n",
      "ru_analogy_tagged.txt\r\n",
      "ruscorpora_mystem_cbow_300_2_2015.bin.gz\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "mkuCawyFX-rW",
    "outputId": "ea312dc3-ebfa-4ac1-b4cb-7b96c6d4ab7e"
   },
   "outputs": [],
   "source": [
    "model_path = 'ruscorpora_mystem_cbow_300_2_2015.bin.gz'\n",
    "\n",
    "model_ru = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H4D0hMgkX-rY"
   },
   "outputs": [],
   "source": [
    "words = ['день_S', 'ночь_S', 'человек_S', 'семантика_S', 'биткоин_S']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jbTShU9aX-ra"
   },
   "source": [
    "Частеречные тэги нужны, поскольку это специфика скачанной модели - она была натренирована на словах, аннотированных их частями речи (и лемматизированных). **Важно!** В названиях моделей на `rusvectores` указано, какой набор тегов они используют (mystem, upos и т.д.)\n",
    "\n",
    "Попросим у модели 10 ближайших соседей для каждого слова и коэффициент косинусной близости для каждого:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9Uer5sHyX-ra",
    "outputId": "b5eec20b-995c-42e7-855c-388ded24682c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "день_S\n",
      "неделя_S :  0.7165195941925049\n",
      "месяц_S :  0.631048858165741\n",
      "вечер_S :  0.5828739404678345\n",
      "утро_S :  0.5676207542419434\n",
      "час_S :  0.5605547428131104\n",
      "минута_S :  0.5297019481658936\n",
      "гекатомбеон_S :  0.4897990822792053\n",
      "денек_S :  0.48224714398384094\n",
      "полчаса_S :  0.48217129707336426\n",
      "ночь_S :  0.478074848651886\n",
      "\n",
      "\n",
      "ночь_S\n",
      "вечер_S :  0.6946247816085815\n",
      "утро_S :  0.57301926612854\n",
      "ноченька_S :  0.5582467317581177\n",
      "рассвет_S :  0.5553582906723022\n",
      "ночка_S :  0.5351512432098389\n",
      "полдень_S :  0.5334426164627075\n",
      "полночь_S :  0.478694349527359\n",
      "день_S :  0.4780748784542084\n",
      "сумерки_S :  0.4390218257904053\n",
      "фундерфун_S :  0.4340824782848358\n",
      "\n",
      "\n",
      "человек_S\n",
      "женщина_S :  0.5979775190353394\n",
      "парень_S :  0.4991787374019623\n",
      "мужчина_S :  0.4767409563064575\n",
      "мужик_S :  0.47384002804756165\n",
      "россиянин_S :  0.47190436720848083\n",
      "народ_S :  0.4654741883277893\n",
      "согражданин_S :  0.45378512144088745\n",
      "горожанин_S :  0.44368088245391846\n",
      "девушка_S :  0.44314485788345337\n",
      "иностранец_S :  0.43849867582321167\n",
      "\n",
      "\n",
      "семантика_S\n",
      "семантический_A :  0.5334584712982178\n",
      "понятие_S :  0.5030269622802734\n",
      "сочетаемость_S :  0.4817051291465759\n",
      "актант_S :  0.47596412897109985\n",
      "хронотоп_S :  0.46330299973487854\n",
      "метафора_S :  0.46158894896507263\n",
      "мышление_S :  0.4610119163990021\n",
      "парадигма_S :  0.45796656608581543\n",
      "лексема_S :  0.45688074827194214\n",
      "смысловой_A :  0.4543077349662781\n",
      "\n",
      "\n",
      "Увы, слова \"биткоин_S\" нет в модели!\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    # есть ли слово в модели? \n",
    "    if word in model_ru:\n",
    "        print(word)\n",
    "        # выдаем 10 ближайших соседей слова:\n",
    "        for word, sim in model_ru.most_similar(positive=[word], topn=10):\n",
    "            # слово + коэффициент косинусной близости\n",
    "            print(word, ': ', sim)\n",
    "        print('\\n')\n",
    "    else:\n",
    "        # Увы!\n",
    "        print(f'Увы, слова \"{word}\" нет в модели!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'биткоин_S' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-86f583aeefee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_ru\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'биткоин_S'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Python/3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'биткоин_S' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model_ru['биткоин_S']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1GqDmAcJX-rc"
   },
   "source": [
    "Находим косинусную близость пары слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "MDFjOSJjX-rd",
    "outputId": "b58343ff-8063-4cca-8cbb-371cbf4717ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23895611\n"
     ]
    }
   ],
   "source": [
    "print(model_ru.similarity('человек_S', 'обезьяна_S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B0w4pQooX-rf"
   },
   "source": [
    "Построим аналогии: \n",
    "* *король: мужчина = королева: женщина* \n",
    " $\\Rightarrow$ \n",
    "* *король - мужчина + женщина = королева*\n",
    "\n",
    "\n",
    "Что получится, если вычесть из пиццы Италию и прибавить Сибирь?\n",
    "\n",
    "* positive — вектора, которые мы складываем\n",
    "* negative — вектора, которые вычитаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "N0L5_TCQX-rf",
    "outputId": "18c0c545-9843-4325-82bb-a3614e0ac023"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "пельмень_S\n"
     ]
    }
   ],
   "source": [
    "print(model_ru.most_similar(positive=['пицца_S', 'сибирь_S'], negative=['италия_S'])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "DrN2Jc31X-rh",
    "outputId": "55ec81c1-8fa2-4998-e094-bca1a2aae04d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ананас_S'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ru.doesnt_match('пицца_S пельмень_S хот-дог_S ананас_S'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобученные модели: GloVe\n",
    "\n",
    "Скачаем модель с сайта: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_embeddings-Copy1.ipynb\r\n",
      "2_embeddings_no_output.ipynb\r\n",
      "2_embeddings_with_output.ipynb\r\n",
      "IMDB Dataset.csv\r\n",
      "alice.txt\r\n",
      "cc.en.300.bin\r\n",
      "cc.en.300.bin.gz\r\n",
      "cc.ru.300.bin\r\n",
      "cc.ru.300.bin.gz\r\n",
      "clean_text.txt\r\n",
      "\u001b[34mglove.6B\u001b[m\u001b[m\r\n",
      "glove.6B.zip\r\n",
      "imdb-dataset-of-50k-movie-reviews.zip\r\n",
      "movie_reviews.model\r\n",
      "movies_alice.bin\r\n",
      "ru_analogy_tagged.txt\r\n",
      "ruscorpora_mystem_cbow_300_2_2015.bin.gz\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\n",
      ", -0.10767 0.11053 0.59812 -0.54361 0.67396 0.10663 0.038867 0.35481 0.06351 -0.094189 0.15786 -0.81665 0.14172 0.21939 0.58505 -0.52158 0.22783 -0.16642 -0.68228 0.3587 0.42568 0.19021 0.91963 0.57555 0.46185 0.42363 -0.095399 -0.42749 -0.16567 -0.056842 -0.29595 0.26037 -0.26606 -0.070404 -0.27662 0.15821 0.69825 0.43081 0.27952 -0.45437 -0.33801 -0.58184 0.22364 -0.5778 -0.26862 -0.20425 0.56394 -0.58524 -0.14365 -0.64218 0.0054697 -0.35248 0.16162 1.1796 -0.47674 -2.7553 -0.1321 -0.047729 1.0655 1.1034 -0.2208 0.18669 0.13177 0.15117 0.7131 -0.35215 0.91348 0.61783 0.70992 0.23955 -0.14571 -0.37859 -0.045959 -0.47368 0.2385 0.20536 -0.18996 0.32507 -1.1112 -0.36341 0.98679 -0.084776 -0.54008 0.11726 -1.0194 -0.24424 0.12771 0.013884 0.080374 -0.35414 0.34951 -0.7226 0.37549 0.4441 -0.99059 0.61214 -0.35111 -0.83155 0.45293 0.082577\n",
      "\u001b[K.019169 0.41479 -0.34349 0.26872 0.04464 0.42131 -0.41032 0.15459 0.022239 -0.64:\u001b[Klove.6B/glove.6B.100d.txt\u001b[m\u001b[K\u0007"
     ]
    }
   ],
   "source": [
    "! less ./glove.6B/glove.6B.100d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "glove = {}\n",
    "with open('./glove.6B/glove.6B.200d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        word, embedding = line.split(' ',1)\n",
    "        wordEmbedding = np.array([float(value) for value in embedding[1:].split(' ')])\n",
    "        glove[word] = wordEmbedding\n",
    "\n",
    "print(len(glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove['hello'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобученные модели: fastText\n",
    "\n",
    "FastText использует не только векторы слов, но и векторы n-грам. В корпусе каждое слово автоматически представляется в виде набора символьных n-грамм. Скажем, если мы установим n=3, то вектор для слова \"where\" будет представлен суммой векторов следующих триграм: \"<wh\", \"whe\", \"her\", \"ere\", \"re>\" (где \"<\" и \">\" символы, обозначающие начало и конец слова). Благодаря этому мы можем также получать вектора для слов, отсутствуюших в словаре, а также эффективно работать с текстами, содержащими ошибки и опечатки.\n",
    "\n",
    "* [Статья](https://aclweb.org/anthology/Q17-1010)\n",
    "* [Сайт](https://fasttext.cc/)\n",
    "* [Руководство](https://fasttext.cc/docs/en/support.html)\n",
    "* [Репозиторий](https://github.com/facebookresearch/fasttext)\n",
    "\n",
    "Есть библиотека `fasttext` для питона (с готовыми моделями можно работать и через `gensim`).\n",
    "\n",
    "На сайте проекта можно найти предобученные модели для 157 языков (в том числе русского): https://fasttext.cc/docs/en/crawl-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "#fasttext.util.download_model('ru', if_exists='ignore')\n",
    "ft = fasttext.load_model('cc.ru.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.90226841e-02,  2.94893887e-02,  7.75389597e-02,  5.80755584e-02,\n",
       "       -2.04796474e-02, -2.60778125e-02, -4.31554951e-02, -7.66336499e-03,\n",
       "        7.35045224e-03,  8.39629862e-03, -3.03267990e-03,  6.59276769e-02,\n",
       "        2.52222875e-03, -1.30695077e-02,  4.03538942e-02,  1.97116341e-02,\n",
       "        7.49906823e-02,  3.26768160e-02, -1.30058741e-02,  5.22828884e-02,\n",
       "        5.06116338e-02, -7.52337575e-02, -3.59434858e-02,  4.87469845e-02,\n",
       "        1.21515250e-05,  2.44456176e-02, -6.05776981e-02,  3.70115340e-02,\n",
       "        3.57847698e-02, -1.98335946e-02,  5.92223294e-02, -6.81746677e-02,\n",
       "       -1.24129206e-02, -3.59787606e-02, -4.27804003e-03,  4.76003997e-02,\n",
       "       -7.42428824e-02, -1.28765211e-01, -1.36825100e-01,  9.66134109e-03,\n",
       "       -5.21074906e-02,  1.38343694e-02, -2.73327827e-02,  5.51195256e-02,\n",
       "        2.06264984e-02,  5.95376752e-02,  2.22954024e-02,  7.45679426e-04,\n",
       "       -5.00567667e-02,  4.27671038e-02, -2.68213451e-02,  1.15477806e-02,\n",
       "       -2.37321910e-02,  3.29448245e-02, -3.59060019e-02, -6.77625239e-02,\n",
       "        1.04473326e-02,  1.50889102e-02, -2.78114676e-02, -9.76028666e-02,\n",
       "       -3.43934074e-02, -1.04172770e-02, -3.95480804e-02,  8.23004544e-03,\n",
       "        4.43044715e-02, -5.16277030e-02, -1.58495817e-03,  1.30589223e-02,\n",
       "        2.92139989e-03,  8.11357237e-03,  2.42509153e-02,  1.58560872e-02,\n",
       "        1.11486539e-02,  3.76834609e-02,  1.30302114e-02, -3.58502977e-02,\n",
       "        3.56120542e-02, -5.51831089e-02, -2.32887710e-03,  4.76860767e-03,\n",
       "       -4.78371978e-03,  2.99577117e-02, -3.09092756e-02, -6.36521801e-02,\n",
       "        2.03362219e-02,  2.66498933e-03, -8.36646780e-02, -7.17537245e-03,\n",
       "        8.98520052e-02,  6.66208640e-02, -8.96188617e-02,  5.34536690e-03,\n",
       "       -9.03579406e-03,  8.92835259e-02,  2.78102700e-02, -1.20203774e-02,\n",
       "       -2.41292827e-03, -1.15023619e-02, -3.44534665e-02, -2.43556350e-02,\n",
       "       -7.05607096e-03, -4.11733650e-02,  1.92692038e-02,  1.19909272e-03,\n",
       "       -1.21868670e-03, -5.36188670e-02,  5.19845784e-02,  2.51053344e-03,\n",
       "       -6.82635009e-02,  1.17374798e-02, -9.05148908e-02,  4.99660224e-02,\n",
       "        1.12783313e-01,  1.24725781e-03, -2.74479073e-02, -8.96990314e-05,\n",
       "        6.92080036e-02,  1.45684648e-03, -2.41988879e-02, -3.05815246e-02,\n",
       "        5.17066428e-03,  7.69108161e-03, -5.95876239e-02, -3.95577922e-02,\n",
       "       -5.22434339e-03,  3.09095122e-02, -2.98223142e-02, -7.57112056e-02,\n",
       "        4.94429357e-02,  5.03483489e-02,  4.14576894e-03, -3.37062441e-02,\n",
       "       -3.80479768e-02, -3.73744667e-02,  7.57561484e-03,  2.64757015e-02,\n",
       "        3.25574283e-03,  2.18765642e-02,  4.24088500e-02, -3.50630693e-02,\n",
       "        5.54031208e-02, -8.53787642e-03,  1.19881807e-02, -1.30064994e-01,\n",
       "       -5.70928156e-02, -7.49739036e-02, -4.37675416e-02, -2.35290714e-02,\n",
       "        4.50193062e-02, -9.09404084e-03, -4.47827242e-02, -1.77210849e-02,\n",
       "       -1.05877578e-01, -2.54368186e-02, -1.35848140e-02,  1.31518561e-02,\n",
       "        9.84252430e-03,  1.03610352e-01,  8.95494297e-02,  8.20680708e-03,\n",
       "        5.17309010e-02,  3.53422575e-02,  2.93518547e-02, -2.68315640e-03,\n",
       "        2.45563984e-02, -1.37855113e-02, -8.25848505e-02, -2.55577751e-02,\n",
       "        6.90167472e-02, -1.15719941e-02, -5.48847876e-02, -1.04961231e-01,\n",
       "       -2.70559490e-02, -3.56582142e-02, -5.01041189e-02, -2.91199442e-02,\n",
       "       -2.19394565e-02, -6.56976998e-02, -3.12608741e-02, -3.23637240e-02,\n",
       "       -7.50813708e-02,  8.37855563e-02,  4.47793305e-02, -2.74285376e-02,\n",
       "        4.54183817e-02, -2.78103482e-02,  6.49284497e-02, -2.38500033e-02,\n",
       "       -6.63511455e-03,  9.03142020e-02,  4.88595525e-03,  5.21496907e-02,\n",
       "       -5.44826463e-02, -1.13993289e-03,  8.48236308e-02, -5.48246168e-02,\n",
       "        1.53390333e-01, -5.71697950e-04,  1.42700663e-02, -1.30275860e-02,\n",
       "        8.65012687e-03,  1.22714564e-02,  1.69300865e-02,  2.29607075e-02,\n",
       "       -7.44452775e-02,  1.51818572e-02,  7.68800527e-02, -6.26277253e-02,\n",
       "        1.95666961e-02, -5.96546941e-02,  2.62541715e-02,  2.89816558e-02,\n",
       "        5.54927848e-02,  1.97875500e-02,  1.77371211e-03,  7.34284669e-02,\n",
       "        4.84075435e-02,  8.13406426e-03, -3.19512337e-02, -3.59821431e-02,\n",
       "        2.43038242e-03, -2.33888905e-02, -7.13486224e-02,  3.22470479e-02,\n",
       "        6.24932116e-03,  3.05196736e-02, -7.63035566e-02,  2.74270605e-02,\n",
       "       -2.94636339e-02,  7.15403408e-02,  4.15845476e-02,  5.25651593e-03,\n",
       "       -7.12789819e-02,  1.43155335e-02, -8.60781893e-02,  5.22206712e-04,\n",
       "       -6.64970353e-02, -1.76892467e-02, -1.44125810e-02, -1.12688266e-01,\n",
       "       -7.95892905e-03,  2.14078669e-02,  2.14393754e-02,  5.67282131e-03,\n",
       "        3.80397029e-02,  4.51862812e-02, -5.92934974e-02,  2.56342590e-02,\n",
       "       -1.34147201e-02, -2.59316415e-02,  7.45109171e-02,  3.16312276e-02,\n",
       "       -1.19421585e-02,  1.85775813e-02,  1.57254320e-02,  1.73833873e-02,\n",
       "       -6.95936903e-02, -4.40367265e-03, -3.95593718e-02, -1.94147956e-02,\n",
       "        4.49333750e-02, -1.81070566e-02, -5.35898767e-02,  4.37325016e-02,\n",
       "        6.83941832e-03, -3.00847143e-02, -1.11508211e-02,  4.33934405e-02,\n",
       "       -1.91740710e-02,  7.54076838e-02, -4.10673320e-02,  5.50695919e-02,\n",
       "        5.69503196e-02, -3.72967497e-02,  9.86716524e-03, -2.55110618e-02,\n",
       "       -2.46207975e-02, -2.20296606e-02, -2.08327528e-02,  9.03372467e-03,\n",
       "       -8.93854573e-02, -1.17132045e-01, -2.37822570e-02,  2.06042416e-02,\n",
       "       -4.70974110e-02,  4.29784134e-03,  2.73348554e-03, -3.65018807e-02,\n",
       "       -5.95521331e-02, -2.10720743e-03,  8.33550282e-03, -6.47981092e-02,\n",
       "        7.08063394e-02, -2.68884492e-03, -4.61591482e-02,  2.17411760e-02,\n",
       "       -7.07311854e-02,  6.36055395e-02,  1.93226598e-02,  1.78099480e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft['модель']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У fasttext есть все те же методы, что в gensim, но называются они иначе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.764227032661438, 'кофе'),\n",
       " (0.739784836769104, 'Чай'),\n",
       " (0.7071998119354248, 'чая'),\n",
       " (0.7018463015556335, 'чаи'),\n",
       " (0.6877091526985168, 'свежезаваренный'),\n",
       " (0.6864805221557617, 'напиток'),\n",
       " (0.6854358911514282, 'каркадэ'),\n",
       " (0.6772830486297607, 'чай.'),\n",
       " (0.667264461517334, 'чай-'),\n",
       " (0.6647352576255798, 'чаёк')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.get_nearest_neighbors('чай')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8757159113883972, 'актриса'),\n",
       " (0.7068515419960022, 'артистка'),\n",
       " (0.694389820098877, 'киноактриса'),\n",
       " (0.6874823570251465, 'Актриса'),\n",
       " (0.6860095262527466, 'кинозвезда'),\n",
       " (0.6789741516113281, 'певица'),\n",
       " (0.6663230061531067, 'красавица-актриса'),\n",
       " (0.6603893637657166, 'актриса.'),\n",
       " (0.6569409966468811, 'Киноактриса'),\n",
       " (0.6488985419273376, 'актрисса')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.get_analogies(\"женщина\", \"мужчина\", \"актер\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важная особенность: так как модель обучена на символьных n-граммах, нет проблемы OOV (out of vocabulary) слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7441761493682861, 'книженция'),\n",
       " (0.7169104218482971, 'книжица'),\n",
       " (0.6966618895530701, 'брошюрка'),\n",
       " (0.6812918782234192, 'книжонку'),\n",
       " (0.6812120079994202, 'книжонок'),\n",
       " (0.6565592288970947, 'книжонки'),\n",
       " (0.6502901911735535, 'книжонке'),\n",
       " (0.6479876637458801, 'книжечка'),\n",
       " (0.6242943406105042, 'статейка'),\n",
       " (0.6214005351066589, 'книга-то')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.get_nearest_neighbors('книжонка')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7335842251777649, 'компъютер'),\n",
       " (0.7109572291374207, 'компютера'),\n",
       " (0.7098245024681091, 'компьютер'),\n",
       " (0.6971184015274048, 'компютерный'),\n",
       " (0.6874867081642151, 'копьютер'),\n",
       " (0.674636960029602, 'копм'),\n",
       " (0.6739663481712341, 'Компютер'),\n",
       " (0.6691707968711853, 'компутер'),\n",
       " (0.6670798063278198, 'компютеру'),\n",
       " (0.6656312346458435, 'комп')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.get_nearest_neighbors('компютер')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZNHMtmbqX-rl"
   },
   "source": [
    "## Как обучить свою модель: word2vec\n",
    "\n",
    "В качестве обучающих данных возьмем англоязычные отзывы о фильмах с сайта IMDB (данные взяты отсюда http://ai.stanford.edu/~amaas/data/sentiment/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "B2wv-vSxX-ro",
    "outputId": "d32ef7a3-ced2-4581-8b0f-b59d624ab8a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "eEGmrh-nX-rq",
    "outputId": "46b6d027-0919-48b6-9b92-9ce895d572d6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.&lt;br /&gt;&lt;br /&gt;The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.&lt;br /&gt;&lt;br /&gt;It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangst...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. &lt;br /&gt;&lt;br /&gt;The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. &lt;br /&gt;&lt;br /&gt;The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on ou...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.&lt;br /&gt;&lt;br /&gt;This was the most I'd laughed at one of Woody's comedies in years (dare I say a decade?). While I've never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.&lt;br /&gt;&lt;br /&gt;This may not be the crown jewel of h...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet &amp; his parents are fighting all the time.&lt;br /&gt;&lt;br /&gt;This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.&lt;br /&gt;&lt;br /&gt;OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing &amp; arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.&lt;br /&gt;&lt;br /&gt;3 out of 10 just for the well playing parents &amp; descent dialogs. As for the shots with Jake: just ignore them.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. &lt;br /&gt;&lt;br /&gt;This being a variation on the Arthur Schnitzler's play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.&lt;br /&gt;&lt;br /&gt;The only thing one gets out of all these ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            review sentiment\n",
       "0  One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangst...  positive\n",
       "1  A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on ou...  positive\n",
       "2  I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.<br /><br />This was the most I'd laughed at one of Woody's comedies in years (dare I say a decade?). While I've never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />This may not be the crown jewel of h...  positive\n",
       "3                                                     Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a variation on the Arthur Schnitzler's play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.<br /><br />The only thing one gets out of all these ...  positive"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oIZaQ3kYX-rt"
   },
   "source": [
    "Убираем из данных ссылки, html-разметку и небуквенные символы, а затем приведем все к нижнему регистру и токенизируем. На выходе получается массив из предложений, каждое из которых представляет собой массив слов. Здесь используется токенизатор из библиотеки `nltk`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import Pool\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "irbunSOfX-rt"
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(review, remove_stopwords=False ):\n",
    "    # убираем ссылки вне тегов\n",
    "    review = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \" \", review)\n",
    "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    words = review_text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = stopwords.words(\"english\")\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)\n",
    "\n",
    "def review_to_sentences(review, tokenizer=tokenizer, remove_stopwords=False):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce393af890bb4782823916dd15190a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "50000\n",
      "[['one', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', 'oz', 'episode', 'you', 'll', 'be', 'hooked'], ['they', 'are', 'right', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me', 'the', 'first', 'thing', 'that', 'struck', 'me', 'about', 'oz', 'was', 'its', 'brutality', 'and', 'unflinching', 'scenes', 'of', 'violence', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go'], ['trust', 'me', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid'], ['this', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', 'sex', 'or', 'violence'], ['its', 'is', 'hardcore', 'in', 'the', 'classic', 'use', 'of', 'the', 'word', 'it', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'oswald', 'maximum', 'security', 'state', 'penitentary'], ['it', 'focuses', 'mainly', 'on', 'emerald', 'city', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda'], ['em', 'city', 'is', 'home', 'to', 'many', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'and', 'more', 'so', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away', 'i', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'wouldn', 't', 'dare'], ['forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance', 'oz', 'doesn', 't', 'mess', 'around'], ['the', 'first', 'episode', 'i', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', 'i', 'couldn', 't', 'say', 'i', 'was', 'ready', 'for', 'it', 'but', 'as', 'i', 'watched', 'more', 'i', 'developed', 'a', 'taste', 'for', 'oz', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence'], ['not', 'just', 'violence', 'but', 'injustice', 'crooked', 'guards', 'who', 'll', 'be', 'sold', 'out', 'for', 'a', 'nickel', 'inmates', 'who', 'll', 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', 'well', 'mannered', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', 'watching', 'oz', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', 'thats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side']] [['a', 'wonderful', 'little', 'production'], ['the', 'filming', 'technique', 'is', 'very', 'unassuming', 'very', 'old', 'time', 'bbc', 'fashion', 'and', 'gives', 'a', 'comforting', 'and', 'sometimes', 'discomforting', 'sense', 'of', 'realism', 'to', 'the', 'entire', 'piece'], ['the', 'actors', 'are', 'extremely', 'well', 'chosen', 'michael', 'sheen', 'not', 'only', 'has', 'got', 'all', 'the', 'polari', 'but', 'he', 'has', 'all', 'the', 'voices', 'down', 'pat', 'too'], ['you', 'can', 'truly', 'see', 'the', 'seamless', 'editing', 'guided', 'by', 'the', 'references', 'to', 'williams', 'diary', 'entries', 'not', 'only', 'is', 'it', 'well', 'worth', 'the', 'watching', 'but', 'it', 'is', 'a', 'terrificly', 'written', 'and', 'performed', 'piece'], ['a', 'masterful', 'production', 'about', 'one', 'of', 'the', 'great', 'master', 's', 'of', 'comedy', 'and', 'his', 'life'], ['the', 'realism', 'really', 'comes', 'home', 'with', 'the', 'little', 'things', 'the', 'fantasy', 'of', 'the', 'guard', 'which', 'rather', 'than', 'use', 'the', 'traditional', 'dream', 'techniques', 'remains', 'solid', 'then', 'disappears'], ['it', 'plays', 'on', 'our', 'knowledge', 'and', 'our', 'senses', 'particularly', 'with', 'the', 'scenes', 'concerning', 'orton', 'and', 'halliwell', 'and', 'the', 'sets', 'particularly', 'of', 'their', 'flat', 'with', 'halliwell', 's', 'murals', 'decorating', 'every', 'surface', 'are', 'terribly', 'well', 'done']] [['i', 'thought', 'this', 'was', 'a', 'wonderful', 'way', 'to', 'spend', 'time', 'on', 'a', 'too', 'hot', 'summer', 'weekend', 'sitting', 'in', 'the', 'air', 'conditioned', 'theater', 'and', 'watching', 'a', 'light', 'hearted', 'comedy'], ['the', 'plot', 'is', 'simplistic', 'but', 'the', 'dialogue', 'is', 'witty', 'and', 'the', 'characters', 'are', 'likable', 'even', 'the', 'well', 'bread', 'suspected', 'serial', 'killer'], ['while', 'some', 'may', 'be', 'disappointed', 'when', 'they', 'realize', 'this', 'is', 'not', 'match', 'point', 'risk', 'addiction', 'i', 'thought', 'it', 'was', 'proof', 'that', 'woody', 'allen', 'is', 'still', 'fully', 'in', 'control', 'of', 'the', 'style', 'many', 'of', 'us', 'have', 'grown', 'to', 'love', 'this', 'was', 'the', 'most', 'i', 'd', 'laughed', 'at', 'one', 'of', 'woody', 's', 'comedies', 'in', 'years', 'dare', 'i', 'say', 'a', 'decade'], ['while', 'i', 've', 'never', 'been', 'impressed', 'with', 'scarlet', 'johanson', 'in', 'this', 'she', 'managed', 'to', 'tone', 'down', 'her', 'sexy', 'image', 'and', 'jumped', 'right', 'into', 'a', 'average', 'but', 'spirited', 'young', 'woman', 'this', 'may', 'not', 'be', 'the', 'crown', 'jewel', 'of', 'his', 'career', 'but', 'it', 'was', 'wittier', 'than', 'devil', 'wears', 'prada', 'and', 'more', 'interesting', 'than', 'superman', 'a', 'great', 'comedy', 'to', 'go', 'see', 'with', 'friends']]\n"
     ]
    }
   ],
   "source": [
    "with Pool(4) as p:\n",
    "    sentences = list(tqdm(p.imap(review_to_sentences, data[\"review\"]), total=len(data)))\n",
    "\n",
    "print(len(sentences))\n",
    "print(*sentences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536641"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_sentences = [item for sublist in sentences for item in sublist]\n",
    "len(flat_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cqb_NOteX-rz"
   },
   "outputs": [],
   "source": [
    "# это понадобится нам позже для обучения модели fasttext\n",
    "\n",
    "with open('clean_text.txt', 'w') as f:\n",
    "    for s in flat_sentences:\n",
    "        f.write(' '.join(s))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qqFhuv7XX-r1"
   },
   "source": [
    "Обучаем и сохраняем модель. \n",
    "\n",
    "\n",
    "Основные параметры:\n",
    "* данные должны быть итерируемым объектом \n",
    "* size — размер вектора, \n",
    "* window — размер окна наблюдения,\n",
    "* min_count — мин. частотность слова в корпусе,\n",
    "* sg — используемый алгоритм обучения (0 — CBOW, 1 — Skip-gram),\n",
    "* sample — порог для downsampling'a высокочастотных слов,\n",
    "* workers — количество потоков,\n",
    "* alpha — learning rate,\n",
    "* iter — количество итераций,\n",
    "* max_vocab_size — позволяет выставить ограничение по памяти при создании словаря (т.е. если ограничение превышается, то низкочастотные слова будут выбрасываться). Для сравнения: 10 млн слов = 1Гб RAM.\n",
    "\n",
    "**Важно!** Обратите внимание, что тренировка модели не включает предобработку! Это значит, что избавляться от пунктуации, приводить слова к нижнему регистру, лемматизировать их, проставлять частеречные теги придется до тренировки модели (если, конечно, это необходимо для вашей задачи). Т.е. в каком виде слова будут в исходном тексте, в таком они будут и в модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "OB2FqIioX-r1",
    "outputId": "7a8840a9-a12b-4247-bf6b-b02c767ce2c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "CPU times: user 2min 38s, sys: 1.15 s, total: 2min 40s\n",
      "Wall time: 46.4 s\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "\n",
    "%time model_en = word2vec.Word2Vec(flat_sentences, workers=4, size=300, min_count=10, window=10, sample=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HFU6SYAZX-r3"
   },
   "source": [
    "Смотрим, сколько в модели слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HA3P0CUYX-r4",
    "outputId": "1bfec39d-aa30-4a4e-b77c-e8ed9c8c2571"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27864\n"
     ]
    }
   ],
   "source": [
    "print(len(model_en.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2mU9sbGYX-r8"
   },
   "source": [
    "Попробуем оценить модель вручную, аналогично тому, как проверяли предобученные модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "n8OQxmUyX-r9",
    "outputId": "4e4ab20e-31e4-4c02-fdfa-61b4ba8be133"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('actress', 0.7952963709831238)]\n"
     ]
    }
   ],
   "source": [
    "print(model_en.wv.most_similar(positive=[\"woman\", \"actor\"], negative=[\"man\"], topn=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('men', 0.5924724340438843)]\n"
     ]
    }
   ],
   "source": [
    "print(model_en.wv.most_similar(positive=[\"dogs\", \"man\"], negative=[\"dog\"], topn=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('europe', 0.7917618751525879), ('germany', 0.7751280665397644), ('canada', 0.774277925491333)]\n"
     ]
    }
   ],
   "source": [
    "print(model_en.wv.most_similar(\"usa\", topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "novel\n"
     ]
    }
   ],
   "source": [
    "print(model_en.wv.doesnt_match(\"comedy thriller western novel\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Почему важно, на каких данных обучалась модель?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на ближайшие по смыслу слова к слову \"star\":\n",
    "- в модели, обученной на обзорах фильмов\n",
    "- в модели, обученной на Википедии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('stars', 0.6092053651809692)\n",
      "('hudson', 0.4575274586677551)\n",
      "('singer', 0.446683406829834)\n",
      "('stardom', 0.4426514506340027)\n",
      "('fame', 0.4368395209312439)\n",
      "('starred', 0.41369011998176575)\n",
      "('studded', 0.4064015746116638)\n",
      "('starring', 0.4028608202934265)\n",
      "('tyrone', 0.4026487469673157)\n",
      "('icon', 0.3995055556297302)\n"
     ]
    }
   ],
   "source": [
    "print(*model_en.wv.most_similar(\"star\", topn=10), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38832128"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_en.similarity('star', 'celebrity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16640092"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_en.similarity('star', 'sky')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20186296"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_en.similarity('star', 'shine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем предобученную модель fastText для английского:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext.util\n",
    "#fasttext.util.download_model('en', if_exists='ignore') \n",
    "\n",
    "ft_eng = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44439566]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([ft_eng['star']], [ft_eng['celebrity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29363436]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([ft_eng['star']], [ft_eng['sky']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26306346]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([ft_eng['star']], [ft_eng['shine']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "juR4NX0wX-r_"
   },
   "source": [
    "### Как дообучить существующую модель\n",
    "\n",
    "При тренировке модели \"с нуля\" веса инициализируются случайно, однако можно использовать для инициализации векторов веса из предобученной модели, таким образом как бы дообучая ее.\n",
    "\n",
    "Сначала посмотрим близость какой-нибудь пары слов в имеющейся модели, чтобы потом сравнить результат с дообученной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3138753"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_en.wv.similarity('white', 'rabbit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mxsz8DKMX-sD"
   },
   "source": [
    "В качестве дополнительных данных для обучения возьмем английский текст «Алисы в Зазеркалье»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "yHuXJgB_X-sI",
    "outputId": "ee69966b-8428-4172-ba4c-7669a57465ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['through', 'the', 'looking-glass', 'by', 'lewis', 'carroll', 'chapter', 'i', 'looking-glass', 'house', 'one', 'thing', 'was', 'certain', 'that', 'the', 'white', 'kitten', 'had', 'had', 'nothing', 'to', 'do', 'with', 'it', '', 'it', 'was', 'the', 'black', 'kitten’s', 'fault', 'entirely'], ['for', 'the', 'white', 'kitten', 'had', 'been', 'having', 'its', 'face', 'washed', 'by', 'the', 'old', 'cat', 'for', 'the', 'last', 'quarter', 'of', 'an', 'hour', 'and', 'bearing', 'it', 'pretty', 'well', 'considering', 'so', 'you', 'see', 'that', 'it', 'couldn’t', 'have', 'had', 'any', 'hand', 'in', 'the', 'mischief']]\n"
     ]
    }
   ],
   "source": [
    "with open(\"alice.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = re.sub('\\n', ' ', text)\n",
    "sents = sent_tokenize(text)\n",
    "\n",
    "punct = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~„“«»†*—/\\-‘’'\n",
    "clean_sents = []\n",
    "\n",
    "for sent in sents:\n",
    "    s = [w.lower().strip(punct) for w in sent.split()]\n",
    "    clean_sents.append(s)\n",
    "    \n",
    "print(clean_sents[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dH8f7GNBX-sK"
   },
   "source": [
    "Чтобы дообучить модель, надо сначала ее сохранить, а потом загрузить. Все параметры тренировки (размер вектора, мин. частота слова и т.п.) будут взяты из загруженной модели, т.е. задать их заново нельзя.\n",
    "\n",
    "**NB!** Дообучить можно только полную модель, а `KeyedVectors` — нельзя. Поэтому сохранять модель нужно в соотвествующем формате. Подробнее о разнице [вот тут](https://radimrehurek.com/gensim/models/keyedvectors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "Rv--44TmX-sL",
    "outputId": "0973b4a0-dcbe-498e-86a5-28f6e9673aa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "model_path = \"movie_reviews.model\"\n",
    "\n",
    "print(\"Saving model...\")\n",
    "model_en.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "f6eZwBQzX-sQ",
    "outputId": "be412d2b-8cc7-424b-9251-bc2aedb1474d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97278, 150225)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = word2vec.Word2Vec.load(model_path)\n",
    "\n",
    "model.build_vocab(clean_sents, update=True)\n",
    "model.train(clean_sents, total_examples=model.corpus_count, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nMLhISWiX-sS"
   },
   "source": [
    "\"Белый\" и \"кролик\" стали ближе друг к другу!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33047336"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('white', 'rabbit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Классификация текстов</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сегодня мы подробнее познакомимся с задачей классификации текстов на два или более классов.\n",
    "\n",
    "У нас есть [набор данных отзывов на фильмы IMDB](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews), который часто используют при разработке и оценке моделей классификации текстов. Каждый из отзывов в наборе данных имеет свою оценку: является ли он позитивным или негативным.\n",
    "\n",
    "**Цель:** Используя данные отзывов IMDB, построить модель, которая на основе текста отзыва сможет определять, относится ли он к позитивнму или к негативному классу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## План работы\n",
    "* Подготовка данных\n",
    "* Классификация с помощью Word2Vec:\n",
    "    * Обучение модели Word2Vec\n",
    "    * Построение векторов для текстов через среднее векторов слов\n",
    "    * Построение векторов для текстов через средневзвешенное векторов слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения нейросетевых моделей (далее) мы будем использовать популярный фреймворк [PyTorch](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NHKLq9hEFhNr"
   },
   "source": [
    "В предположении, что PyTorch уже установлен, поставим дополнительные модули (torchvision и torchtext) и загрузим модель для токенизации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "5kiZro9eG-bG",
    "outputId": "f500b2c7-8b1d-45a0-a737-07ffb8179e3b"
   },
   "outputs": [],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "a-WRgs-VFhNt",
    "outputId": "adb907ba-5a3e-462d-c671-228b4031de02"
   },
   "outputs": [],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет IMDB из torchtext, это будет удобно для дальнейшего использования этих данных при обучении нейронной сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n",
      "downloading aclImdb_v1.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score,classification_report,confusion_matrix\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter,defaultdict\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "TEXT = data.Field(tokenize='spacy')\n",
    "LABEL = data.LabelField()\n",
    "\n",
    "train_src, test = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на обучаующую и тестовую выборку и преобразуем их в формат, удобный для построения классических моделей машинного обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [example.text for example in train_src]\n",
    "y_train = [example.label for example in train_src]\n",
    "\n",
    "X_test = [example.text for example in test]\n",
    "y_test = [example.label for example in test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train examples 25000\n",
      "total test examples 25000\n",
      "12500 positive and 12500 negative examples in test\n",
      "12500 positive and 12500 negative examples in test\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print (\"total train examples %s\" % len(y_train))\n",
    "print (\"total test examples %s\" % len(y_test))\n",
    "\n",
    "train_counter = Counter(y_train)\n",
    "test_counter = Counter(y_test)\n",
    "\n",
    "print (\"{0} positive and {1} negative examples in test\".format(test_counter['pos'],test_counter['neg']))\n",
    "print (\"{0} positive and {1} negative examples in test\".format(test_counter['pos'],test_counter['neg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно видеть, что классы сбалансированы: количество позитивных и негативных отзывов совпадают."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на примеры позитивного и негативного отзыва."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative example:\n",
      "For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem . Imagine a movie where Joe Piscopo is actually funny ! Maureen Stapleton is a scene stealer . The Moroni character is an absolute scream . Watch for Alan \" The Skipper \" Hale jr . as a police Sgt .\n",
      "\n",
      "\n",
      "Positive example:\n",
      "Not that I dislike childrens movies , but this was a tearjerker with few redeeming qualities . M.J. Fox was the perfect voice for Stuart and the rest of the talent was wasted . Hugh Laurie can be amazingly funny , but is not given the chance in this movie . It´s sugar - coated sugar and would hardly appeal to anyone over 7 years of age . See Toy Story , Monsters Inc. or Shrek instead . 3/10\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Negative example:\\n{0}\\n\\n\".format(' '.join(X_train[0])))\n",
    "print(\"Positive example:\\n{0}\\n\\n\".format(' '.join(X_train[-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация с помощью Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробно языковая модель *Word2Vec* была рассмотрена на предыдущей неделе курса.\n",
    "\n",
    "Здесь мы будем использовать эту модель для получения векторных представлений слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# # model = Word2Vec(X_train, size=100, window=5, min_count=5, workers=4)\n",
    "# # model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы решить задачу классификации текстов, нам нужно каждому тексту поставить в соответствие вектор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем класс *MeanEmbeddingVectorizer*, с помощью которого будем получать вектора для текстов отзывов на основе векторов слов.\n",
    "Вектор каждого текста будет равен среднему векторов входящих в него слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(list(word2vec.values())[0])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем также немного другой подход $-$ будем получать вектор для текста как средневзвешенное векторов входящих в него слов с весами *tf-idf*.\n",
    "\n",
    "Для этого реализуем класс *TfidfEmbeddingVectorizer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(list(word2vec.values())[0])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем модель Word2Vec в словарь для дальнейшего использования для векторизации текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = dict(zip(w2v_model.wv.index2word, w2v_model.wv.syn0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для классификации будем использовать модель [случайного леса](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) из scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc_w2v = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
    "    (\"extra trees\", RandomForestClassifier(n_estimators=20))])\n",
    "rfc_w2v_tfidf = Pipeline([\n",
    "    (\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)),\n",
    "    (\"extra trees\", RandomForestClassifier(n_estimators=20))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.03 s, sys: 56.2 ms, total: 8.09 s\n",
      "Wall time: 8.12 s\n"
     ]
    }
   ],
   "source": [
    "%time rfc_w2v.fit(X_train,y_train)\n",
    "pred = rfc_w2v.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим отчет по предсказаниям модели (с помощью MeanEmbeddingVectorizer) на тестовой выборке. Выведем точность (precision), полноту (recall) и f1-меру для позитивного и негативного классов, а также количество объектов каждого класса (support) в тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:   0.60\n",
      "Recall:   0.60\n",
      "F1-measure:   0.60\n",
      "Accuracy:   0.60\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.59      0.67      0.63     12500\n",
      "         pos       0.62      0.53      0.57     12500\n",
      "\n",
      "    accuracy                           0.60     25000\n",
      "   macro avg       0.60      0.60      0.60     25000\n",
      "weighted avg       0.60      0.60      0.60     25000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_test, pred, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_test, pred, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_test, pred, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_test, pred)))\n",
    "print(classification_report(y_test, pred))\n",
    "labels = rfc_w2v.classes_\n",
    "\n",
    "\n",
    "sns.heatmap(data=confusion_matrix(y_test, pred), annot=True, fmt=\"d\", cbar=False, xticklabels=labels, yticklabels=labels)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае классы сбалансированы (количество объектов позитивного и негативного классов совпадают), поэтому в качестве метрики качества можно использовать *accuracy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 44s, sys: 1.19 s, total: 1min 45s\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%time rfc_w2v_tfidf.fit(X_train,y_train)\n",
    "pred = rfc_w2v_tfidf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь выведем результаты работы классификатора на признаках из *TfidfEmbeddingVectorizer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:   0.62\n",
      "Recall:   0.62\n",
      "F1-measure:   0.62\n",
      "Accuracy:   0.62\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.61      0.68      0.64     12500\n",
      "         pos       0.64      0.56      0.60     12500\n",
      "\n",
      "    accuracy                           0.62     25000\n",
      "   macro avg       0.62      0.62      0.62     25000\n",
      "weighted avg       0.62      0.62      0.62     25000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_test, pred, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_test, pred, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_test, pred, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_test, pred)))\n",
    "print(classification_report(y_test, pred))\n",
    "labels = rfc_w2v.classes_\n",
    "\n",
    "\n",
    "sns.heatmap(data=confusion_matrix(y_test, pred), annot=True, fmt=\"d\", cbar=False, xticklabels=labels, yticklabels=labels)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно видеть, что при использовании *TfidfEmbeddingVectorizer* метрика Accuracy немного увеличилась по сравнению c *MeanEmbeddingVectorizer* $-$ с 0.60 до 0.62."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
